{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_uGG1HeL-L5a"},"outputs":[],"source":["#Carga de librerias\n","import os, re, json\n","import pandas as pd\n","from pandas import DataFrame\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from gensim.corpora import Dictionary\n","from itertools import chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsLngBy9-L5h"},"outputs":[],"source":["#Varaibles Globales\n","STOPWORDS_PATH = os.path.join(\"stopwords\")\n","DATOS_PATH = \"../datos/Subtrack1-Scientific_Literature/Train/training_set_subtrack1_only_articles.json\"\n","NOMBRE_COLUMNA_TEXTO = \"abstractText\"\n","NOMBRE_COLUMNA_DOCUMENTOS = \"docs\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rEE8Hi80-L5i"},"outputs":[],"source":["#Definicion de funciones\n","def cargar_datos() -> DataFrame:\n","    with open(DATOS_PATH, encoding=\"utf-8\") as f:\n","        datos = json.load(f)\n","        \n","    datos = pd.json_normalize(datos, record_path=\"articles\")\n","    return datos\n","\n","\n","def quitar_numeros(texto:str) -> str:\n","    return re.sub(\"([0-9])\", \"\",texto)\n","\n","\n","def pasar_a_minusculas(texto:str) -> str:\n","    return texto.lower()\n","\n","\n","def tokenizar_palabras(texto:str) -> str:\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    return tokenizer.tokenize(texto)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZLeK1VH-L5j"},"outputs":[],"source":["#Carga de stopwords y remoción de stopwords\n","def cargar_stopwords() -> list[str]:\n","    lista_stopwords = []\n","\n","    for archivo in os.listdir(STOPWORDS_PATH):\n","        if archivo.endswith(\".txt\"):\n","            with open(os.path.join(STOPWORDS_PATH, archivo), mode=\"r\", encoding=\"utf-8\") as f:\n","                stpwords = f.readlines()\n","                lista_stopwords+=[word.replace(\"\\n\", \"\") for word in stpwords]\n","\n","    lista_stopwords = lista_stopwords + stopwords.words('spanish')\n","    lista_stopwords = list(set(lista_stopwords))\n","\n","    lista_stopwords.sort(key=lambda x: len(x))\n","\n","    return lista_stopwords\n","\n","\n","def quitar_stopwords(texto:list[str], stopwords:list[str]) -> list[str]:\n","    texto = set(texto)\n","    stpwords = set(stopwords)\n","\n","    return list(texto.difference(stpwords))"]},{"cell_type":"markdown","metadata":{"id":"WoUAsdDS-L5l"},"source":["TO DO 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyVt4n7o-L5n"},"outputs":[],"source":["#TODO lematización y/o stemming\n","#TODO bigramas y trigramas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gE1BjxF-L5p"},"outputs":[],"source":["#Pipeline bag of words\n","def pipeline_bag_of_words(datos: DataFrame, guardar: bool=True, no_below:int=None, no_above=None) -> dict:\n","    dictionary = Dictionary(datos[NOMBRE_COLUMNA_DOCUMENTOS].to_list())\n","    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n","    # TODO explorar TFidf vectorizer\n","    \n","    if no_below and no_above:\n","        dictionary.filter_extremes(no_below, no_above)\n","    \n","    corpus = [dictionary.doc2bow(doc) for doc in datos[NOMBRE_COLUMNA_DOCUMENTOS]]\n","    result = {\n","        \"corpus\": corpus,\n","        \"dictionary\": dictionary,\n","        \"nro_tokens_unicos\": len(dictionary),\n","        \"nro_documentos\": len(corpus)\n","    }\n","    bow = dictionary.doc2bow(list(chain(*datos[NOMBRE_COLUMNA_DOCUMENTOS].to_list())))\n","    \n","    frecuencias = dict()\n","\n","    id2token = dictionary.id2token\n","\n","    if not id2token:\n","        # En caso de bug en el que id2token retorne un diccionario vacio\n","        # Se crea a partir de token2id\n","        id2token = {v: k for k ,v in dictionary.token2id.items()}\n","    \n","    for key, freq in bow:\n","        frecuencias[id2token.get(key)] = freq\n","    \n","    # Ordena de mayor a menor frecuencia\n","    frecuencias = dict(sorted(frecuencias.items(), key=lambda x:x[1], reverse=True))\n","\n","    result[\"id2token\"] = id2token\n","    result[\"frecuencias\"] = frecuencias\n","\n","    if guardar:\n","        with open(\"bag_of_word_freqs.json\", mode=\"w\", encoding=\"utf-8\") as f:\n","            f.writelines(json.dumps(frecuencias, ensure_ascii=False, indent=4))\n","        print(\"Archivo guardado\")\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfDi4R7K-L5r"},"outputs":[],"source":["#Pipeline data prep nlp\n","def pipeline_data_prep_nlp(texto:str, stpwords:list[str]) -> str:\n","    # print(\"Quitando números\")\n","    texto = quitar_numeros(texto)\n","    # print(\"Pasando texto a minúsculas\")\n","    texto = pasar_a_minusculas(texto)\n","    # print(\"Tokenizando palabras\")\n","    texto = tokenizar_palabras(texto)\n","    # print(\"Quitando stopwords\")\n","    texto = quitar_stopwords(texto, stpwords)\n","\n","    return texto"]},{"cell_type":"markdown","metadata":{"id":"Cl28eZES-L5t"},"source":["# TO DO 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neMAZfRJ-L5u"},"outputs":[],"source":["#TODO agregar creación de gráficos\n","def plot_freqs_bow():\n","    # https://stackoverflow.com/questions/36262748/python-save-plotly-plot-to-local-file-and-insert-into-html\n","    pass\n","\n","def plot_wordcloud():\n","    # https://github.com/amueller/word_cloud\n","    #TODO para todo los documentos al mismo tiempo\n","    #TODO para cada tópico\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4mTKwVf-L5v"},"outputs":[],"source":["#Entrenamiento de modelo\n","def entrenar_lda(rdo_pipe_bow:dict, save_path:str=None):\n","    #TODO Considerar el tiempo como una variable\n","    # https://markroxor.github.io/gensim/static/notebooks/ldaseqmodel.html\n","    from gensim.models import LdaModel\n","    \n","    # Set training parameters.\n","    num_topics = 10\n","    chunksize = 2000\n","    passes = 20\n","    iterations = 400\n","    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n","\n","    modelo = LdaModel(\n","        corpus=rdo_pipe_bow[\"corpus\"],\n","        id2word=rdo_pipe_bow[\"id2token\"],\n","        chunksize=chunksize,\n","        alpha='auto',\n","        eta='auto',\n","        iterations=iterations,\n","        num_topics=num_topics,\n","        passes=passes,\n","        eval_every=eval_every\n","    )\n","\n","    # top_topics = modelo.top_topics(rdo_pipe_bow[\"corpus\"])\n","\n","    # # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n","    # avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n","    # print('Average topic coherence: %.4f.' % avg_topic_coherence)\n","\n","    # from pprint import pprint\n","    # pprint(top_topics)\n","\n","    return modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3h8tf-c-L5w"},"outputs":[],"source":["#Funcion  que hace correr el modelo - esto hace correr todo lo demas.\n","def run()-> DataFrame:\n","    print(\"Cargando datos\")\n","    df = cargar_datos()\n","    print(\"Cargando Stopwords\")\n","    stpwords = cargar_stopwords()\n","    print(\"Iniciando pipeline NLP\")\n","    df[NOMBRE_COLUMNA_DOCUMENTOS] = df.apply(lambda fila: pipeline_data_prep_nlp(fila[NOMBRE_COLUMNA_TEXTO], stpwords), axis=1)\n","    print(\"Calculando bag of words\")\n","    rdo_pipe_bow = pipeline_bag_of_words(df, no_below=20, no_above=0.50, guardar=False)\n","    rdo_pipe_bow[\"dictionary\"].save(\"./diccionarios/diccionario_gensim_1\")\n","    print(\"Entrenando lda\")\n","    modelo = entrenar_lda(rdo_pipe_bow)\n","    modelo.save(\"./modelos/LDA_gensim_1\")\n","\n","    return df\n","\n","\n","if __name__ == \"__main__\":\n","    run()"]},{"cell_type":"code","source":["modelo_final = run()"],"metadata":{"id":"Kg5HgIH0wKiX"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"name":"2022_05_22_pipeline_nlp_notebook.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}